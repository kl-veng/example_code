{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4e8f7ee-0592-413b-8d92-5fbab81a7f9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text classification of text regions in newspapers as a job ad or non-job ad using BERT classifier\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e110eb58-9ecf-4d71-ab60-c1e87adaa333",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices:\n",
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"
     ]
    }
   ],
   "source": [
    "print(\"Available devices:\")\n",
    "for device in tf.config.list_physical_devices():\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2009497-4ec1-4ba1-b094-6a08042aae43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_empty_words(tokens):\n",
    "    tokens = list(filter(None, str(tokens)))\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d56b6668-ccfa-4b17-b251-38727a103ff8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def mask_inputs_for_bert(texts, max_len, tokenizer):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    for text in tqdm(texts):\n",
    "        encoded_dict = tokenizer.encode_plus(text, add_special_tokens = True, max_length = max_len, pad_to_max_length = True, return_attention_mask = True, is_split_into_words=False)\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "    input_ids = tf.convert_to_tensor(input_ids)\n",
    "    attention_masks = tf.convert_to_tensor(attention_masks)\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6697ee5-7ed5-4c16-b6e9-7dcd5c144fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(list_of_text_data):\n",
    "    for text in tqdm(list_of_text_data):\n",
    "        text = remove_empty_words(text)\n",
    "    return list_of_text_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9d766ab-6652-49bc-be52-1830c3cae5fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = 'google-bert/bert-base-german-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "09ca637b-76e4-4da2-8efd-53eed4725387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained(model_name, num_labels=2, from_pt=True)\n",
    "model_save_path = f'bert_model_{model_name[:5]}.weights.h5'\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path, save_weights_only=True, monitor='val_loss', mode='min', save_best_only=True), tf.keras.callbacks.TensorBoard()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57edb98f-3c57-4c81-805b-e451f03054cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5, epsilon=1e-08)\n",
    "bert_model.compile(loss=loss, optimizer=optimizer, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49c890a4-7982-46e0-8d68-dad1016a2874",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('job_ads_identification_df.csv', encoding='utf-8')\n",
    "df = df.dropna(subset=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47cac167-0cdd-437a-ad0a-b5d40fec722e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_positive = df[df['job_ad'] == 1]\n",
    "df_negative = df[df['job_ad'] == 0]\n",
    "\n",
    "df_positive_sampled = df_positive.sample(n=2500, random_state=42)\n",
    "df_negative_sampled = df_negative.sample(n=2500, random_state=42)\n",
    "\n",
    "df_sampled = pd.concat([df_positive_sampled, df_negative_sampled])\n",
    "df_sampled = df_sampled.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16bc5b90-f17c-435f-bfe1-e293bfb05efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df_sampled['text']  \n",
    "y = df_sampled['job_ad'] \n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b0ea510-9e37-41c5-9a56-70ddcc40fc3c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 360265.76it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 480392.17it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train = preprocess(X_train)\n",
    "X_test = preprocess(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4c36f22-bd6d-460c-bdc3-a8872b45d5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b7eaaa7-874c-4d83-be5c-c5e414fb2327",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_input = X_train.values\n",
    "train_label = y_train.values\n",
    "\n",
    "test_input = X_test.values\n",
    "test_label = y_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aacd7168-042c-4a13-a4a9-721bd447064f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "c:\\Users\\venglaro\\Documents\\example_code\\venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2829: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|██████████| 4000/4000 [00:00<00:00, 8185.81it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 8503.66it/s]\n"
     ]
    }
   ],
   "source": [
    "max_len = 512\n",
    "train_inp, train_mask = mask_inputs_for_bert(train_input, max_len, tokenizer)\n",
    "test_inp, test_mask = mask_inputs_for_bert(test_input, max_len, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b9a6989e-9323-48e4-b04d-752e5d12d48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_label = tf.convert_to_tensor(train_label)\n",
    "test_label = tf.convert_to_tensor(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73ded614-64fc-4457-969c-346bef363032",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "250/250 [==============================] - 3903s 16s/step - loss: 0.4646 - accuracy: 0.7922 - val_loss: 0.3629 - val_accuracy: 0.8460\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\venglaro\\Documents\\example_code\\venv\\Lib\\site-packages\\transformers\\generation\\tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n",
      "  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250/250 [==============================] - 4780s 19s/step - loss: 0.2884 - accuracy: 0.8932 - val_loss: 0.3623 - val_accuracy: 0.8630\n",
      "Epoch 3/5\n",
      "250/250 [==============================] - 3990s 16s/step - loss: 0.2058 - accuracy: 0.9312 - val_loss: 0.4289 - val_accuracy: 0.8340\n",
      "Epoch 4/5\n",
      "250/250 [==============================] - 3799s 15s/step - loss: 0.1541 - accuracy: 0.9498 - val_loss: 0.4628 - val_accuracy: 0.8480\n",
      "Epoch 5/5\n",
      "250/250 [==============================] - 3731s 15s/step - loss: 0.1135 - accuracy: 0.9610 - val_loss: 0.5340 - val_accuracy: 0.8480\n"
     ]
    }
   ],
   "source": [
    "history = bert_model.fit([train_inp, train_mask], train_label, batch_size=16, epochs=5, validation_data=([test_inp, test_mask], test_label), callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4ee64a2f-ef1b-4d5f-829d-eb1c73e0905a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 6199.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 301s 9s/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85       524\n",
      "           1       0.81      0.89      0.85       476\n",
      "\n",
      "    accuracy                           0.85      1000\n",
      "   macro avg       0.85      0.85      0.85      1000\n",
      "weighted avg       0.85      0.85      0.85      1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "test_input = X_test.values\n",
    "test_label = y_test.values\n",
    "test_inp, test_mask = mask_inputs_for_bert(test_input, max_len, tokenizer)\n",
    "test_label = tf.convert_to_tensor(test_label)\n",
    "\n",
    "pred_raw = bert_model.predict([test_inp, test_mask])\n",
    "pred = pred_raw[0].argmax(axis = 1)\n",
    "\n",
    "print(classification_report(test_label, pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
