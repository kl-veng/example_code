{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Part: OCR\n",
    "Import Image from PIL library to load images and import pytesseract library. Don't forget to specify path to tesseract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = 'C:/Users/venglaro/Tesseract-OCR/tesseract.exe'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load image you want to OCR. We'll start with the Little Prince image which is in Czech:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('C:/Users/venglaro/Desktop/OCR_teaching/images/maly_princ.jpeg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify which model you want to use and do the recognition.\n",
    "\n",
    "Hint: Not sure, which model to use? On https://tesseract-ocr.github.io/tessdoc/Data-Files, you'll find the list of languages and their tesseract models.\n",
    "\n",
    "Hint2: Getting error while OCRing? Make sure that you downloaded the language model you want to use from https://github.com/tesseract-ocr/tessdata and moved it into your Tesseract-OCR/tessdata folder. You only need to do this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_model = 'ces'\n",
    "text = pytesseract.image_to_string(img, lang=ocr_model)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part: How good is my result?\n",
    "\n",
    "Load the file contatining ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_path = 'C:/Users/venglaro/Desktop/OCR_teaching/transcription/maly_princ.txt'\n",
    "with open(ground_truth_path, 'r', encoding='utf-8') as file:\n",
    "    ground_truth = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Levenshtein modul (if not installed: pip install Levenshtein) and calculate the distance.\n",
    "\n",
    "Brief recap: Levenshtein distance = number of editing operations (insert, delete, replace) we need to perform in order to obtain identical strings.\n",
    "\n",
    "Note: Is OCR boring for you? Invest some effort into these evaluation metrics anyway! You can use Levenshtein, CER and WER anyway in many other applications! For example to evaluate of your speech-to-text results, or use texts similarity to detect plagiats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "distance = Levenshtein.distance(text, ground_truth)\n",
    "print(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it say about the similarity of texts? Is it a lot or a few? Is every second character mis-recognized, or every 100th? Let's calculate text similarity by taking into account the length of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_text_similarity(distance, text1, text2):\n",
    "    if max(len(text1), len(text2)) > 0:\n",
    "        similarity = 1 - (distance / max(len(text1), len(text2)))\n",
    "    else:\n",
    "        similarity = None\n",
    "    return similarity\n",
    "\n",
    "similarity = count_text_similarity(distance, text, ground_truth)\n",
    "print(similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides Levenshtein distance, there are also other metrics such as Character Error Rate (CER; how many characters from 100 are mis-recognized - the lower the better), or Word Error Rate (WER; on the words level).\n",
    "Their calculation is based on Levenshtein distance and is defined as (S + D + I) / N, where S is number of substitutions, D number of deletions, I number of insertions and N number of characters (CER) or words (WER) in the ground truth.\n",
    "\n",
    "Luckily, we just need to undestand the concepts, but the metrics were already implemented by other people and are available in python packages. \n",
    "\n",
    "If not installed: **pip install jiwer**\n",
    "\n",
    "Note: How do I choose appropriate metric?\n",
    "It depends on your use-case! Generally, CER or Levenshtein distance are fitting when OCRing normal text, WER e.g. when OCRing insurance numbers where you absolutely need the token as unit to be correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jiwer \n",
    "\n",
    "wer = jiwer.wer(text, ground_truth)\n",
    "print(f'WER: {wer}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cer = jiwer.cer(text, ground_truth)\n",
    "print(f'CER: {cer}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Part: Post-processing\n",
    "\n",
    "Post-processing (or post-correction) serves to improvement of the OCR result. There are many ways to do that, from rule-based or dictionary approaches to BERT and machine learning. How to choose the correct approach depends on the type of errors, OCR quality, amount of data or availability of pre-trained post-processing models or availability of training data if we want to train a post-correction model ourselves.\n",
    "\n",
    "In our OCRed text, one source of imperfectness are additional multiple new lines. Replacing multiple new lines by only one could improve our results. (Btw, this would be one of the rule-based approaches.) \n",
    "\n",
    "Did the distance/similarity change after this step?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_lines(input_string):\n",
    "    while '\\n\\n' in input_string:\n",
    "        input_string = input_string.replace('\\n\\n', '\\n')\n",
    "    return input_string\n",
    "\n",
    "filtered_text = filter_empty_lines(text)\n",
    "\n",
    "distance = Levenshtein.distance(filtered_text, ground_truth)\n",
    "similarity = count_text_similarity(distance, filtered_text, ground_truth)\n",
    "print(f'New distance is: {distance}, text similarity is now {similarity}.')\n",
    "\n",
    "wer = jiwer.wer(filtered_text, ground_truth)\n",
    "cer = jiwer.cer(filtered_text, ground_truth)\n",
    "print(f'New WER is: {wer}, new CER is: {cer}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Part: Your turn ðŸ˜Š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the ibn_18640702_010.jpg image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define (and download) appropriate model and do the OCR.\n",
    "\n",
    "Hint: the data is in German, the font is called Fraktur. There are several models, feel free to use any of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Load the ground truth image (in the transcriptions folder, named ONB_ibn_18640702_010.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Select one or several evaluation metrics and find out how good your OCR result is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Part: Post-corrections with BERT-based model\n",
    "\n",
    "Depending on the model you used, the OCR errors are different. What they probably have in common is that the errors are too complex to define hand-crafted rules. For these cases, machine-learning approaches can be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a pre-trained model fine-tuned on our data from https://huggingface.co/Var3n/hmByT5_anno. To use huggingface models, you need to do all the necessary installations.\n",
    "\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import and define the tokenizer and model we want to use. For the first time, the model will be download. By all other uses, it will only be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Var3n/hmByT5_anno\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Var3n/hmByT5_anno\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of issues is that the data needs to be divided into batches with maximal length so they can be processed. For our model, it's 350 characters (see the documentation). As our text is already seperated by new lines, it seems convenient to use these lines as inputs. In the end, we will check whether the longest lines is still shorter than the maximum size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_by_new_line(text):\n",
    "    tokens = re.split(\"\\\\n\", text)\n",
    "    tokens = list(filter(None, tokens))\n",
    "    return tokens\n",
    "\n",
    "lines = split_by_new_line(text)\n",
    "print(f'The maximum length of a line is {max(len(line) for line in lines)}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will process each of the lines. The tokenizer turns the text in the number that can be processed by the model. In the end, we get the text processed by the model and decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_postprocessed = []\n",
    "\n",
    "for line in lines:\n",
    "    inp = tokenizer(line, return_tensors=\"pt\").input_ids\n",
    "    output = model.generate(inp, max_new_tokens=len(inp[0]), num_beams=4, do_sample=True)\n",
    "    postprocessed_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    result_postprocessed.append(postprocessed_text)\n",
    "\n",
    "postprocessed = '\\n'.join(result_postprocessed)\n",
    "print(postprocessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we look at the postprocessed text and the ground truth, we can see some characters we don't want, namely the long s ('Å¿') instead of 's'. Let's replace them to obtain results more similar to our ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessed = postprocessed.replace('Å¿', 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the metrics for the text before and after the post-processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance = Levenshtein.distance(text, ground_truth)\n",
    "similarity = count_text_similarity(distance, text, ground_truth)\n",
    "wer = jiwer.wer(text, ground_truth)\n",
    "cer = jiwer.cer(text, ground_truth)\n",
    "\n",
    "print('Results for the raw OCR:')\n",
    "print(f'Distance is: {distance}, text similarity is {similarity}.')\n",
    "print(f'The WER is: {wer}, CER is: {cer}.\\n')\n",
    "\n",
    "distance = Levenshtein.distance(postprocessed, ground_truth)\n",
    "similarity = count_text_similarity(distance, postprocessed, ground_truth)\n",
    "wer = jiwer.wer(postprocessed, ground_truth)\n",
    "cer = jiwer.cer(postprocessed, ground_truth)\n",
    "\n",
    "print('Results for the post-corrected text:')\n",
    "print(f'New distance is: {distance}, text similarity is now {similarity}.')\n",
    "print(f'New WER is: {wer}, new CER is: {cer}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: It can happen, that your results are actually worse after the post-processing. In this case, it's a good idea to check the type of errors in your original OCRed text and the post-processed and decide, what's better for your use-case.\n",
    "\n",
    "Different solutions than tesseract exist, e.g. EasyOCR (https://github.com/JaidedAI/EasyOCR). The choice depends on your application, available language models and personal preferences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
